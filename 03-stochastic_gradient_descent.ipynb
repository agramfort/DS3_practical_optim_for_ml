{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (SGD) on Logistic Regression\n",
    "\n",
    "Author: Alexandre Gramfort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of content\n",
    "\n",
    "[1. Loss functions, gradients and step-sizes](#loss)<br>\n",
    "[2. Generate a dataset](#data)<br>\n",
    "[3. From GD to SGD](#sgd)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import njit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='loss'></a>\n",
    "## Loss functions, gradients and step-sizes\n",
    "\n",
    "We want to minimize\n",
    "$$\n",
    "f(w) = \\frac 1n \\sum_{i=1}^n \\ell(x_i^\\top w, y_i) + \\frac \\lambda 2 \\|w\\|_2^2\n",
    "$$\n",
    "where $\\ell(z, y) = \\log(1 + \\exp(-yz))$ (logistic regression).\n",
    "\n",
    "We write it as a minimization problem of the form\n",
    "$$\n",
    "f(w) = \\frac 1n \\sum_{i=1}^n f_i(w)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "f_i(w) = \\ell(x_i^\\top w, y_i) + \\frac \\lambda 2 \\|w\\|_2^2.\n",
    "$$\n",
    "\n",
    "The gradient of f_i reads:\n",
    "$$\n",
    "\\nabla f_i(w) = - \\frac{y_i}{1 + \\exp(y_i x_i^\\top w)} x_i + \\lambda w.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now introduce functions that will be used for the solvers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit    \n",
    "def f_grad_i(i, w, X, y, lambd):\n",
    "    \"\"\"Gradient with respect to a sample\"\"\"\n",
    "    x_i = X[i]\n",
    "    y_i = y[i]\n",
    "    return - x_i * y_i / (1. + np.exp(y_i * np.dot(x_i, w))) + lambd * w\n",
    "\n",
    "\n",
    "@njit\n",
    "def f_grad(w, X, y, lambd):\n",
    "    \"\"\"Full gradient\"\"\"\n",
    "    g = np.zeros_like(w)\n",
    "    n_samples = X.shape[0]\n",
    "    for i in range(n_samples):\n",
    "        g += f_grad_i(i, w, X, y, lambd)\n",
    "    return g / n_samples\n",
    "\n",
    "\n",
    "def f(w, X, y, lambd):\n",
    "    yXw = y * np.dot(X, w)\n",
    "    return np.mean(np.log(1. + np.exp(- yXw))) + lambd * norm(w) ** 2 / 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data'></a>\n",
    "## 2. Generate a dataset\n",
    "\n",
    "We generate datasets for the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal, randn\n",
    "from scipy.linalg.special_matrices import toeplitz\n",
    "\n",
    "\n",
    "def simulate(w, n_samples, std=1., corr=0.5):\n",
    "    \"\"\"Simulation for the logistic regression problem.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    w : ndarray, shape (d,)\n",
    "        The coefficients of the model\n",
    "    n_samples : int\n",
    "        Sample size    \n",
    "    std : float, default=1.\n",
    "        Standard-deviation of the noise\n",
    "    corr : float, default=0.5\n",
    "        Correlation of the features matrix\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : ndarray, shape (n_samples, n_features)\n",
    "        The design matrix.\n",
    "    y : ndarray, shape (n_samples,)\n",
    "        The targets.\n",
    "    \"\"\"    \n",
    "    n_features = w.shape[0]\n",
    "    cov = toeplitz(corr ** np.arange(0, n_features))\n",
    "    X = multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    noise = std * randn(n_samples)\n",
    "    return X, np.sign(X @ w + noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 50\n",
    "n_samples = 10000\n",
    "idx = np.arange(n_features)\n",
    "\n",
    "# Ground truth coefficients of the model\n",
    "w_true = (-1)**idx * np.exp(-idx / 10.)\n",
    "\n",
    "simulate(w_true, n_samples, std=1., corr=0.7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.stem(w_true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerically check loss and gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import check_grad\n",
    "\n",
    "lambd = 1. / n_samples ** (0.5)\n",
    "\n",
    "X, y = simulate(w_true, n_samples, std=1., corr=0.1)\n",
    "\n",
    "# Check that the gradient and the loss numerically match\n",
    "check_grad(f, f_grad, np.random.randn(n_features), X, y, lambd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a very precise minimum to compute distances to minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "w_init = np.zeros(n_features)\n",
    "w_min, f_min, _ = fmin_l_bfgs_b(f, w_init, f_grad,\n",
    "                                args=(X, y, lambd), pgtol=1e-30, factr=1e-30)\n",
    "\n",
    "print(f_min)\n",
    "print(norm(f_grad(w_min, X, y, lambd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sgd'></a> \n",
    "\n",
    "## 3. From Gradient Descent (GD) to Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a class to monitor iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class monitor:\n",
    "    def __init__(self, algo, f, w_min, args=()):\n",
    "        self.w_min = w_min\n",
    "        self.algo = algo\n",
    "        self.f = f\n",
    "        self.args = args\n",
    "        self.f_min = f(w_min, *args)\n",
    "    \n",
    "    def run(self, *algo_args, **algo_kwargs):\n",
    "        t0 = time()\n",
    "        _, w_list = self.algo(*algo_args, **algo_kwargs)\n",
    "        self.total_time = time() - t0\n",
    "        self.w_list = w_list\n",
    "        self.err = [norm(w - self.w_min) for w in w_list]\n",
    "        self.obj = [self.f(w, *self.args) - self.f_min for w in w_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations\n",
    "n_iter = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def gd(w_init, grad, n_iter=100, step=1., args=()):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    w = w_init.copy()\n",
    "    w_list = []\n",
    "    for i in range(n_iter):\n",
    "        w -= step * grad(w, *args)\n",
    "        w_list.append(w.copy())\n",
    "    return w, w_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def lipschitz_logreg(X, y, lambd):\n",
    "    return norm(X, ord=2) ** 2 / (4. * len(X)) + lambd\n",
    "\n",
    "step = 1. / lipschitz_logreg(X, y, lambd)\n",
    "w_init = np.zeros(n_features)\n",
    "monitor_gd = monitor(gd, f, w_min, (X, y, lambd))\n",
    "monitor_gd.run(w_init, f_grad, n_iter, step, args=(X, y, lambd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first numerical comparison of a deterministic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define some plotting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_epochs(monitors, solvers):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for monit in monitors:\n",
    "        plt.semilogy(monit.obj, lw=2)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"$f(w_k) - f(w^*)$\")\n",
    "\n",
    "    plt.legend(solvers)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "\n",
    "    for monit in monitors:\n",
    "        objs = monit.obj\n",
    "        plt.semilogy(np.linspace(0, monit.total_time, len(objs)), objs, lw=2)\n",
    "        plt.xlabel(\"Timing\")\n",
    "        plt.ylabel(\"$f(w_k) - f(w^*)$\")\n",
    "\n",
    "    plt.legend(solvers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "monitors = [monitor_gd]\n",
    "solvers = [\"GD\"]\n",
    "plot_epochs(monitors, solvers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stoc'></a> \n",
    "## 4. Stochastic methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 50\n",
    "\n",
    "# generate indices of random samples\n",
    "iis = np.random.randint(0, n_samples, n_samples * n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD\n",
    "\n",
    "We recall that an iteration of SGD writes\n",
    "\n",
    "- Pick $i$ uniformly at random in $\\{1, \\ldots, n\\}$\n",
    "- Apply\n",
    "$$\n",
    "w_{t+1} \\gets w_t - \\frac{\\eta_0}{\\sqrt{t+1}} \\nabla f_i(w_t)\n",
    "$$\n",
    "\n",
    "where $\\eta_0$ is a step-size to be tuned by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def sgd(w_init, iis, grad_i, n_iter=100, step=1., store_every=n_samples, args=()):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    w = w_init.copy()\n",
    "    w_list = []\n",
    "    for idx in range(n_iter):\n",
    "        i = iis[idx]\n",
    "\n",
    "        w -= step / np.sqrt(idx + 1) * grad_i(i, w, *args) \n",
    "\n",
    "        # Update metrics after each iteration.\n",
    "        if idx % store_every == 0:\n",
    "            w_list.append(w.copy())\n",
    "    return w, w_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step0 = 1e-1\n",
    "w_init = np.zeros(n_features)\n",
    "\n",
    "monitor_sgd = monitor(sgd, f, w_min, (X, y, lambd))\n",
    "monitor_sgd.run(w_init, iis, f_grad_i, n_iter * n_samples, step0, args=(X, y, lambd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "monitors = [monitor_gd, monitor_sgd]\n",
    "solvers = [\"GD\", \"SGD\"]\n",
    "plot_epochs(monitors, solvers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTIONS:</b>\n",
    "     <ul>\n",
    "       <li>Change the value of n_samples and show that SGD becomes even more competitive when n_samples is large.</li>\n",
    "       <li>Change the regularization (the ``lambd`` parameter) to low regularization $\\lambda = 1 / \\textrm{n_samples}$ and high regularization $\\lambda = 1 / \\sqrt{\\textrm{n_samples}}$ and compare your results.</li>\n",
    "       <li>Change the correlation parameter (corr parameter). Show that a high correlation slows down convergencee.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
